\input{preambuloSimple.tex}

%----------------------------------------------------------------------------------------
%	TÍTULO Y DATOS DEL ALUMNO
%----------------------------------------------------------------------------------------

\title{	
\normalfont \normalsize 
\textsc{\textbf{Metaheurística} \\ Doble Grado en Ingeniería Informática y Matemáticas \\ Universidad de Granada} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\Huge Práctica 1\\
\LARGE Algorítmos basados en trayectorias en Problema de Asignación Cuadrática(QAP)
 \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{ Iván Sevillano García \\\\
	DNI: 77187364-P\\ \\
	E-mail: ivansevillanogarcia@correo.ugr.es\\\\
	Grupo del martes, 17:30h-19:30h
	} % Nombre y apellidos

\date{\normalsize\today} % Incluye la fecha actual

%----------------------------------------------------------------------------------------
% DOCUMENTO
%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Muestra el Título

\newpage

\tableofcontents
\newpage

\section{Descripción del problema QAP}

El problema que se nos plantea es el Problema de Asignación Cuadrática(en adelante QAP por las siglas). En él, tenemos una serie de instalaciones las cuales tienen que interactuar entre ellas una cierta "cantidad de trabajo", produciendo un coste. Las instalaciones tienen, además, unas determinadas localizaciones en las que se tienen que situar. Dependiendo de la distancia entre cada dos instalaciones, el coste que producen al interactuar es mayor o menor.\\

Cabe destacar una serie de detalles en el problema:

\begin{itemize}
	\item \textbf{No Euclideo(No Métrico).} Este problema  no pone ninguna objeción a que la distancia de una localización de un lugar a si mismo sea mayor que cero. Además, una instalación puede interactuar consigo misma por consiguiente.
	\item \textbf{No simétrico.} Tampoco pone objeción a que la distancia de una localización a otra no sea la misma que de otra a una.
\end{itemize}

El problema entonces consiste en repartir las instalaciones en las localizaciones de forma que el coste total o trabajo sea mínimo. Esto es fácil de representar con una permutación, que a cada instalación $i$ le asigna una localización $\pi(i)$. Si llamamos $d_{ij}$ a la distancia que hay de la localización $i$ a la $j$, y $f_{ij}$ la cantidad de trabajo que tiene que mandar la instalación $i$ a la $j$, la función de coste asociada al problema sería la siguiente:

\[Coste(\pi)=\sum_{i=1}^{N}\sum_{j=1}^{N}f_{ij}d_{\pi(i)\pi(j)}\]

Si consideramos las matrices de distancias y flujos, $D$ y $F$ y las matrices de que representan a cada permutación y su inversa, $\Pi$ y $\Pi^{-1}$ respectivamente, se puede representar el coste de una manera más sencilla:

\[Coste(\pi)=<F,\Pi^{-1}D\Pi> \]

Donde la aplicación $<,>$ tiene como argumentos dos matrices de mismas dimensiones y se aplica en la suma de la multiplicación de sus componentes una a una. 

\newpage

\section{Breve descripción de los algorítmos utilizados.}
En esta sección vamos a explicar brevemente la configuración de una solución concreta del problema, el funcionamiento de la función de coste y de cómo trabajan los dos algoritmos que se describen en el enunciado.

\subsection{Solución.}

Una solución está perfectamente determinada por un vector de $N$ componentes donde cada una de sus componentes es distinta unas de otras y el rango de valores difiere de 1 hasta N, o lo que es lo mismo, la solución es una permutación. Según la misma, la localización $i$ tendrá alojada la instalación con el número que ocupa en el vector la posición $i$. Para agilizar cálculos, cada permutación, además, guarda su coste asociado. Así, será fácil calcular soluciones vecinas.

\subsection{Función de coste.}

La función de coste es la descrita anteriormente en la primera sección. A continuación el psudo-código. Los parámetros hacen referencia a la matriz de distancias(D), la matriz de flujos(F) y a la permutación que representa nuestra solución:\\

\begin{lstlisting}
Parametros:D,F,P
Output: coste
coste = 0
Para cada elemento de la matriz (i,j):
  coste <- coste + F[j][i]*D[P(j)][P(i)]
\end{lstlisting}


\subsection{Algoritmos basados en trayectorias.}

Esta práctica consistirá en desarrollar cuatro algoritmos basado en trayectorias y un algoritmo híbrido entre dos de ellos. Veamos ahora las descripciones de cada uno de ellos:\\

\subsubsection{Búsqueda multiarranque básico.}

Este algorítmo consiste en utilizar la ya conocida búsqueda local a partir de varios puntos o soluciones de arranque. Puesto que  condición de parada son las mismas evaluaciones, se realizarán menos evaluaciones en cada una de las búsquedas locales del algoritmo. 

\subsubsection{Enfriamiento simulado(ES).}

Este algoritmo se basa en cómo  se comportan una partícula dentro de un metal al enfriarse el mismo. Al principio, esta partícula tiene mucha energía por la alta temperatura, por lo que se mueve mucho aunque vaya a posiciones con más "energía". Conforme se va enfriando el metal, lo mismo le pasa a la partícula y tiene menos energía, por lo que los saltos que hace se empiezan a parecer más a una búsqueda por descenso, intentando ir a posiciones con cada vez menos energía. Pasamos a describir cada una de las características:

\begin{itemize}
	\item \textbf{Aceptación de soluciones.} Este algoritmo se basa en poder aceptar soluciones de peor calidad dependiendo de la temeratura actual. Así, una solución será aceptada cuando:\\
	
	\begin{itemize}
		\item La solución S' es mejor que la solución S actual con una probabilidad de 1.
		\item La solución S' es peor que la solución S actual con una probabilidad de $ e^{\dfrac{-dC}{T_i}}$, donde $dC$ es la diferencia de coste entre la solución actual S y S'. $dC$ es un número positivo ya que S' no mejora a la solución actual. $T_i$ es la temperatura actual. Por tanto, la expresión antes descrita es un número entre 0 y 1 que podremos usar como probabilidad. Además se pueden hacer varias observaciones.\\
		
		La primera es que si la temperatura es muy alta la probabilidad con la que vamos a aceptar una solución es prácticamente 1. Si es muy baja, es casi 0. \\
		
		La segunda, que conforme va avanzando el algoritmo, $dC$ empieza a tener más importancia. Tanto cuando la temperatura es muy alta como cuando es muy baja, $dC$ no afecta demasiado a la aceptación de la solución. Sin embargo, en mitad de la ejecución si es más relevante. Se aceptarán soluciones dependiendo de cuanta diferencia de coste hay entre soluciones.
	
	\end{itemize}
	\item \textbf{Esquema de enfriamiento.} En cada etapa del algoritmo modificaremos la temperatura de la siguiente manera:
	
	\[T_{k+1}=\dfrac{T_k}{1+\beta T_{k}},\beta = \dfrac{T_0-T_f}{MT_0T_f} \]
	 
	 Donde M es el número de enfriamientos. Este es distinto a número de evaluaciones de la función objetivo. Se harán, dentro de una misma etapa de enfriamiento, varias evaluaciones de la función objetivo.
	 
	 \item \textbf{Operador de vecino.} Se utiliza el mismo operador de vecino que en la búsqueda local de la primera práctica. En cada iteración se explorará sólo un vecino que pueda sustituir a la solución actual.
	 
	 \item \textbf{Condición de enfriamiento.} La condición que se quiere considerar en la práctica para enfriar es haber alcanzado un máximo de evaluaciones de nuevos vecinos o haber alcanzado un máximo de vecinos aceptados.
	 
	 \item \textbf{Condición de parada.} El algoritmo finalizará si se llega al máximo de evaluaciones o si en la iteración actual no ha encontrado ninguna solución que hayamos aceptado. Esta última consideración se impone en la práctica pero veremos que pasa si la suprimimos.
	
	
\end{itemize}

Los valores de los parámetros escogidos en esta práctica son los siguientes:\\

\[T_0=\dfrac{\mu C(S_0)}{-ln(\phi)},T_f=10^{-3},\mu=\phi=0.3\]

Se debe comprobar que la temperatura final es menor que la inicial. Como número máximo de vecinos en cada enfriamiento se ha escogido 10 veces el tamaño del problema. Como número de éxitos, la décima parte de este número. Así, el número de enfriamientos será $50000/max\_vecinos$.



\subsubsection{Búsqueda local iterativa(ILS).}

En este algoritmo ejecutaremos de forma repetida una búsqueda local. Cada cierto tiempo, modificaremos una parte significativa de la solución generando una solución vecina más lejana a la vecina de la búsqueda local, y volveremos a aplicar sobre esta nueva solución la búsqueda local. \\

La necesidad de encontrar un vecino más alejado es porque en la búsqueda local ya utilizamos un vecino para descender. Si modificásemos la solución sólo en una trasposición, la búsqueda local nos dará la misma solución. Por ello, en la práctica se nos da la siguiente solución:\\

\begin{itemize}
	\item \textbf{Mutación por sublista.} Por este operador, escogeremos una posición aleatoria $i$ y mezclaremos todos los elementos entre $i$ y $i+t$, siendo t el tamalo de la sublista que  iremos a mezclar. En caso de que la lista sea menor que $i+t$, mezclaremos también los índices que falten del principio de la lista. Para mezclar esta lista, utilizaremos el operador $shuffle$ de la librería $random$ de $python$.
	
	\item \textbf{Mutación por $t$ índices distintos.} En la práctica hemos modificado este operador por el de modificar $t$ índices distintos, sin tener estos que estar contiguos. Esto lo hacemos porque en el QAP la relación entre dos posiciones cualquiera es la misma que la de dos posiciones contiguas. Nuestra forma de generar soluciones lejanas estaría sesgada en tal caso.
\end{itemize}

El tamaño de sublista en nuestro caso entonces será de $t=n/4$, siendo n el tamaño del problema. Aplicaremos el método ILS a 25 soluciones, la primera vez a una solución aleatoria y las siguientes 24 a soluciones mutadas. Mutaremos siempre la mejor solución encontrada.

\subsubsection{Búsqueda por procedimiento greedy aleatorizado(GRASP).}

Este algoritmo se basa en crear soluciones greedy aleatoria y aplicarle la búsqueda local a esta. La solución greedy general es determinista en el sentido de que en cada paso de la creación de la solución, incrusta en la solución el mejor valor. En GRASP, se mete uno de los mejores de forma aleatoria. Así nos aseguramos tener soluciones parcialmente buenas y distintas unas de otras para poder escoger soluciones más diversas sobre las que partir y aplicar la búsqueda local.\\

A continuación se describe la forma de crear soluciones greedy aleatorias iniciales:

\begin{itemize}
	\item \textbf{Primeras dos ciudades.} A nuestra solución todavía por construir comenzamos metiendo, de forma aleatoria, dos de los elementos con mejor puntuación greedy. Esto significa que ordenamos las localizaciones por mejor valor considerado en el algoritmo greedy. Seleccionamos las mejores y, de esas, escogemos dos.
	
	\item \textbf{Siguientes ciudades.} Para las siguientes, se ordenan por el coste relativo de insertar en una localización una instalación concreta. Esto es, cuanto coste añade esta asignación con respecto a las instalaciones ya asignadas:\\
	
	\[C_{rel}(r,s)=\sum_{i \in S_p}f_{r,i}d_{s,S_p(i)}+f_{i,r}d_{S_p(i),s} \]
	
	Se sigue el mismo criterio para seleccionar a las mejores y, de estas, se escoge una asignación aleatoria y se aplica a la solución parcial $S_p$.
\end{itemize}

Las mejores asignaciones a las que nos referimos en la creación de la solución parcial son las que añadan como mucho una cantidad un 30\% peor que la mejor de todas las anteriores. 



\subsubsection{Algoritmo híbrido. ILS-ES}

Esta hibridación utiliza el mismo método de ILS pero con la diferencia de que utiliza el enfriamiento simulado en vez de la búsqueda local. Utilizaremos la misma estructura que en los dos algoritmos por separado con la diferencia de que en el enfriamiento simulado habrá menos pasos de enfriamiento, ya que tendrán menos pasos en cada iteración.

\newpage
\section{Pseudocódigos y explicaciones.}

A continuación se detallan los pseudocódigos de los algoritmos immplementados:\\

\subsection{BMB.}

La búsqueda local que utilizamos es la explicada en la primera práctica (DLB).\\


\noindent\hrulefill

\begin{lstlisting}
Generamos una solucion aleatoria S
Le aplicamos la busqueda local con 5000 pasos.
De 0 a 24:
  Genereamos una solucion aleatoria S'
  Le aplicamos la busqueda local a S'
  Si el coste de S' es menor que la de S
    S = S'
    
Devolvemos S
\end{lstlisting}

\noindent\hrulefill

\subsection{ES.}
Para este algoritmo necesitamos una variable aleatoria U(0,1) a la que llamaremos con $random$.\\

\noindent\hrulefill


\begin{lstlisting}
Generamos S una solucion.
Calculamos los parametros necesarios maxExitos, maxVecinos, M, t0, 
tf y beta descritos anteriormente.
T = t0
Mientras T sea mayor que tf y se hayan realizado exitos 
  exitos = generados = 0
  Mientras exitos < maxExitos y generados < maxVecinos
    Generamos un vecino S' de S.
    generados = generamos + 1
    difCoste = Coste(S')-Coste(S)
    Si difCoste < 0 o random() < exp(-difCoste/T)
      S = S'
      exitos = exitos + 1
  Actualizamos la temperatura actual T = T/(1+beta*T)
    
Devolvemos la mejor solucion que hayamos obtenido.
\end{lstlisting}
\noindent\hrulefill
\newpage

\subsection{ILS.}

En este algoritmo hemos tenido que programar, además del algoritmo, una función de mutación que pasamos a describir en pseudocódigo:\\

\noindent\hrulefill
\begin{lstlisting}
P es la permutacion a mutar.
Mezclamos con shuffle una lista de indices y cogemos los n/4 primeros, L1.
Sacamos y guardamos en L2 los indices marcados de P y los sustituimos por -1 en P.
Mezclamos con shuffle la lista L2.
Sustituimos los -1 en P por los indices de L2 en orden.
\end{lstlisting}
\noindent\hrulefill

El algoritmo ILS:\\

\noindent\hrulefill
\begin{lstlisting}
Generamos una solucion S aleatoria
Le aplicamos la busqueda local con 50000/25 pasos.
Durante 24 veces:
  Mutamos la solucion S y la guardamos en S'.
  Aplicamos busqueda local a S' con 50000/25 pasos.
  Si la solucion S' tiene mejor coste que S
    S = S'

Devolvemos S'
\end{lstlisting}

\subsection{GRASP.}

Este algoritmo tiene varias funciones auxiliares que ayudan a determinar cómo construir la solución greedy aleatorizada. Las funciones a explicar son las siguientes:\\

\textbf{Función $OrdenSuma$}. Ordena las diagonales de una matriz por cuanto vale la suma de su columna y su fila. Devuelve un vector con los índices ya ordenados junto con su coste. Utilizamos la función $sort$ de python para este último propósito. 

\noindent\hrulefill

\begin{lstlisting}
M matriz
L = {} lista de los indices con sus costes
Para i de 0 hasta N
  sumaI = 0
  Para cada elemento de la colummna o la fila que contiene a i,j
    sumaI = sumaI + M[i][j](elemento de la fila)
    sumaI = sumaI + M[j][i](elemento de la columna)
  L <- (i,sumaI)

Devolvemos la lista ordenada por el valor de sumaI.
\end{lstlisting}

\noindent\hrulefill

La segunda función auxiliar se encarga de calcular cuanto añade de costo a la solución parcial $S_p$ con las condiciones iniciales de matrices de flujo y de distancia $F,D$ respectivamente, la asignación de una instalación $r$ no asignada a una localización $s$ no utilizada dentro de la solución:\\

\noindent\hrulefill

\begin{lstlisting}
F,D,Sp,r,s valores de entrada.
coste = 0
Para cada localizacion que ya tenga instalacion asignada
  // Aniadimos a coste lo que cuesta colocar (r,s) con respecto a (i,Sp[i])
  coste = coste + F[r][i]*D[s][Sp[i]]+F[i][r]*D[Sp[i]][s]

Devolvemos coste
\end{lstlisting}

\noindent\hrulefill


La tercera función se encarga de ordenar las posibles inserciones de una instalación  no ubicada en $S_Pp$ en una ubicación no utilizada por $S_p$ ordenadas por el coste de la segunda función auxiliar.

\noindent\hrulefill

\begin{lstlisting}
Tenemos Sp una solucion parcial
L1 = [] lista de posibles inserciones con sus costes
Para cada instalacion i no asignada
  Para cada localizacion j no utilizada
    Aniadimos a L1 la asignacion de la instalacion i en la localizacion j
    junto con su coste
    
Devolvemos la lista L1 por los valores antes calculados.
\end{lstlisting}


\noindent\hrulefill

El pseudocódigo del algoritmo de una iteración del algoritmo GRASP se detalla a continuación. Esta llamada la realizaremos 25 veces, es decir, generaremos 25 soluciones GRASP. El método de escoger dos elementos será mezclar la lista de los mejores y escoger los dos primeros elementos:\\

\noindent\hrulefill
\begin{lstlisting}
Solucion vacia S.
Obtenemos los valores ordenados de las diagonales de la matriz F,Fl
Obtenemos los valores ordenados de las diagonales de la matriz D,Dl
Invertimos Dl(Ya que queremos distancias cortas para flujos grandes)
Calculamos el umbral de mejores uM y peores uP de cada lista.
Nos quedamos en Dl con los indices cuyo valor no sobrepase
  uM+0.3(uP-uM)
Nos quedamos en Fl con los indices cuyo valor no rebaje
  uM-0,3*(uM-uP)

Escogemos aleatoriamente dos elementos de cada lista, f1, f2, d1, d2
Aniadimos a la solucion las asignaciones (f1,d1), (f2,d2)

Para el resto de posibles asignaciones:
  cand es la lista de asignaciones ordenadas calculada para la 
    solucion parcial S.
  Calculamos los umbrales uM, uP para S
  Nos quedamos en cand las asignaciones cuyo valor 
    no sobrepase uM+0.3*(uM-uP)
  Escogemos aleatoriamente una asignacion de cand y
  la aniadimos a la solucion S

Le aplicamos a S la busqueda local con 50000/25 pasos
\end{lstlisting}
\noindent\hrulefill

\subsection{ILS-ES.}

Como hemos dicho, este algoritmo tiene la misma estructura que el ILS pero, en vez de utilizar la búsqueda local, utilizamos enfriamiento simulado para optimizar la solución mutada.

\noindent\hrulefill
\begin{lstlisting}
Generamos una solucion S aleatoria
Le aplicamos la busqueda local con 50000/25 pasos.
Durante 24 veces:
  Mutamos la solucion S y la guardamos en S'.
  Aplicamos enfriamiento simulado a S' con 50000/25 pasos.
  Si la solucion S' tiene mejor coste que S
    S = S'

Devolvemos S'
\end{lstlisting}
\noindent\hrulefill


\newpage

\section{Algoritmo de comparación.}
En esta sección se compararán los resultados de los algoritmos Greedy y búsqueda local. Para dicha comparación, atenderemos a dos estadísticos:\\

\begin{itemize}
	\item \textbf{Desviación a la solución óptima.} Este estadístico evaluará proporcionalmente la diferencia de coste de la mejor solución y la solución obtenida en cada instancia.La fórmula que describe este estadístico es el siguiente:
	\[Desv = \dfrac{1}{ \left| casos\right|}\sum_{i \in casos}100\dfrac{valorAlg_i - mejorVal_i}{mejorVal_i} \]
	
	\item \textbf{Tiempo medio de ejecución.} La media de los tiempos de ejecución de cada algoritmo.
\end{itemize}

Consideraremos entonces que un algoritmo es mejor que otro si la desviación a la solución óptima es menor. En caso de ser iguales, el algoritmo que tarde menos será considerado mejor.

\newpage

\section{Manual de uso.}

Para la implementación de la práctica hemos utilizado el lenguaje precompilado Python3, por lo que debe de estar instalado en el sistema. También hemos hecho uso de la biblioteca random del mismo lenguaje, más en concreto de las funciones randint(inicio,fin), que escoge aleatoriamente un valor entero entre  $inicio$ y $fin-1$, y shuffle(vector), que mezcla el vector que se le pasa como argumento.\\

La forma de utilizar el programa es la siguiente:\\

\begin{itemize}
	\item \textbf{Programas.} Esta practica tiene un total de cinco ejecutables nuevos. Cada ejecutable recibe como argumento el nombre del archivo de entrada con los datos(terminado en '.dat'), que debe de estar en el directorio $./qapdata/$. Tiene que haber también un archivo solución con el mismo nombre pero con terminación '.sln'  en el directorio $./qapsoln/$. Es posible también introducir una semilla como segundo argumento y un fichero de salida:\\
	
	$./nombreAlgoritmo.py$ $./qapdata/nombredatos.dat$ $semilla$ $fichero_salida$\\
	
	Nos dará como resultado:\\
	\begin{itemize}
		\item Solución Algoritmo. La primera linea será la solución del algoritmo en cuestión obtenida. La segunda, cuanto tiempo ha tardado el programa en calcularla y cuanto coste total tiene esta solución.
		\item Mejor solución. La primera linea es la configuración de la mejor solución y la siguiente es su coste.
	\end{itemize}
	
	\item \textbf{Experimento total.} Para ejecutar el experimento completo, nos hemos ayudado de un script que ejecuta todos los casos de prueba y un makefile que lo llama. Así, la forma de obtener todas las soluciones será tan simple como ejecutar el siguiente comando:\\
	
	$make$ $all$\\
	
	Si se quiere ejecutar un algoritmo completo sin los demás, hay que ejecutar:\\
	
	$make$ $todoAlgoritmo$\\
	
	Reemplazando $Algoritmo$ por el nombre del algoritmo. También se pueden obtener los estadísticos ejecutando el siguiente comando después del comando anterior:\\
	
	$make$ $estadisticosAlgoritmo$\\
	
	
	Es importante mencionar que dentro de la ejecución de los programas se les ha introducido una semilla en concreto, $200000$, que no modificaremos.\\
	
	
\end{itemize}

\newpage
\section{Experimento y análisis de resultados.}

Las tablas obtenidas para cada algoritmo son las siguientes:\\


\input{./tablasLatex/BMB.txt}


\input{./tablasLatex/EScutre.txt}


\input{./tablasLatex/ILS.txt}


\input{./tablasLatex/GRASP.txt}


\input{./tablasLatex/ILS-ES.txt}





\newpage
Por último, la tabla de comparación de algoritmos:

\begin{table}[htbp]
	\begin{center}
		\begin{tabular}{|l|l|l|}
			\hline
			Algoritmo &  Desv & Tiempo\\
			\hline \hline
			Greedy & 59.73 & 0,00\\ \hline
			BL & 8.34 & 13.45 \\ \hline
			BMB & 13.29 & 21.31 \\ \hline
			ES & 10.99 & 8.94 \\ \hline
			ILS & 10.35 & 18.65 \\ \hline
			GRASP & 19.67 & 309.15 \\ \hline
			ILS-ES & 10.98 & 33.69 \\ \hline
		\end{tabular}
		\caption{Tabla de comparación}
		\label{tabla:TablaComparacion}
	\end{center}
\end{table}

\newpage

\subsection{Análisis de resultados}

Analizando los estadísticos observamos que ninguno de ellos supera a la búsqueda local en el estadístico Desv. Puesto que algunas de estas metaheurísticas  utilizan por debajo una búsqueda local se nos plantea si realmente son buenas estas nuevas formas de abarcar nuestro problema.\\

Si observamos las instancias de tamaño n pequeño, el algoritmo de multi arranque básico mejora siempre a la solución de la búsqueda. Esto es debido a que el número máximo de evaluaciones sobrepasa de lejos la cantidad de pasos que necesita la búsqueda local para estancarse y no poder encontrar nuevas soluciones. Esto hace que la técnica multi arranque básica sea repetir búsquedas locales casi con la misma cantidad de pasos. Sin embargo, en tamaños más grandes, apenas llegan a conseguir una profundidad adecuada. \\

Si comparamos el enfriamiento simulado con la búsqueda local, vemos que no tenemos el mismo patrón. Se obtienen mejores o peores resultados de forma aleatoria. Sin embargo, en media, obtiene peores resultados. Cabe destacar también que tiene menos tiempo de ejecución.\\

En el algoritmo ILS pasa algo parecido a la búsqueda multi arranque básica. En instancias con un tamaño pequeño, la diversificación de las soluciones y la poca profundidad del problema hace que tenga muy buenos resultados, incluso llegando a  conseguir la solución óptima. Sin embargo, en tamaños de solución más grande, la diversidad de soluciones no compensa a las evaluaciones de menos de cada búsqueda local. Aun así, en algunos tamaños grandes se consigue igualar o superar por  poco a la solución de la búsqueda local. Para mejorar este procedimiento podríamos añadirle evaluaciones a la búsqueda local dentro del ILS y así profundizar más la búsqueda.\\

El algoritmo GRASP tiene un contra muy importante: tarda muchísimo más de lo que tarda cualquier otra metaheurística. Dependiendo de para qué queramos el algoritmo, este será inviable. Esto es debido a que la creación de soluciones greedy aleatorizada necesita mucha capacidad de cómputo(ordena en cada paso muchas listas para obtener el menor y el mayor coste y así escoger los que no sobrepasen el umbral). Por lo demás, tiene un comportamiento parecido a los algorítmos multi arranque, mejores en tamaños pequeños por poder profundizar de la misma manera, y peor en tamaños mayores por falta de la misma. \\

En el algiritmo híbrido ILS-ES tiene el mismo comportamiento multi arranque antes descrito. Puesto que la comparación con la búsqueda local ya se ha comentado, la vamos a comparar con el ILS sin modificación. En tamaños pequeños y por el comportamiento de salto del enfriamiento simulado, suele obtener peores resultados. Sin embargo, conforme el tamaño crece, el enfriamiento simulado le permite salir de óptimos locales dentro de la profundización que se realiza.\\


\subsection{Posibles mejoras de los algoritmos}

Durante la realización de la práctica se han comentado algunas deficiencias. Aquí se recogen algunas ideas para optimizarlas:

\begin{itemize}
	\item \textbf{Profundidad de los algoritmos multi arranque.} Hemos pretendido que nuestras metaheurísticas abarcasen una diversidad demasiado grande en algunos de los casos y demasiado exhaustiva en otros. Una idea para mejorar esto será modificar el número de arranques y la profundidad de la misma. Esto es, cuanto más grande sea el tamaño de nuestro problema, menos multi arranques realizaremos y profundizaremos en cada uno de esos arranques.
	
	\item El algoritmo Greedy, con su intento de aleatorizar el proceso greedy, no tiene en cuenta que el umbral escogido es, en tamaños de n grande, nada óptimo. Ese umbral debería decrecerse cuanto más grande sea el conjunto de asignaciones.
	
\end{itemize}

\end{document}





